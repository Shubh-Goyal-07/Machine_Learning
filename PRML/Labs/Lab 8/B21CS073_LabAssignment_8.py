# -*- coding: utf-8 -*-
"""B21CS073_LabAssignment_8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ibQe6ZPKV_pWpE8ZymeBYc8DdtBCu9wO

# ***QUESTION 1 (Sequential Feature Selection)***

---



---
"""

!pip install mlxtend
!pip install mlxtend --upgrade --no-deps
from mlxtend.feature_selection import SequentialFeatureSelector as SFS

"""### ***1. Preprocess, clean and prepare the dataset based on the previous lab experience. Separate features and labels as X and Y respectively.***"""

import pandas as pd
import numpy as np

data_airline = pd.read_csv('/content/drive/MyDrive/PRML/Lab 8/train.csv')

data_airline

data_airline.info()

data_airline.nunique()

# Replacing NaN in 'Embarked' with mode
delay_mean= (data_airline['Arrival Delay in Minutes'].mean())
print("Mean of Arrival Delay in Minutes is:", delay_mean)
data_airline['Arrival Delay in Minutes'] = data_airline['Arrival Delay in Minutes'].fillna(delay_mean)

data_airline = data_airline.drop(['Unnamed: 0', 'id'], axis=1)

from sklearn import preprocessing
le = preprocessing.LabelEncoder()

data_airline['Gender'] = le.fit_transform(data_airline['Gender'])
data_airline['Customer Type'] = le.fit_transform(data_airline['Customer Type'])
data_airline['Type of Travel'] = le.fit_transform(data_airline['Type of Travel'])
data_airline['Class'] = le.fit_transform(data_airline['Class'])
data_airline['satisfaction'] = le.fit_transform(data_airline['satisfaction'])

data_airline

dataair_y = data_airline['satisfaction']
dataair_x = data_airline.drop(['satisfaction'], axis=1)

"""### ***2. Create an object of SFS by embedding the Decision Tree classifier object, providing 10 features, forward as True, floating as False and scoring = accuracy. Train SFS and report accuracy for all 10 features. Also, list the names of the 10 best features selected by SFS.***"""

from sklearn.tree import DecisionTreeClassifier as DTC

clf = DTC()
sfs = SFS(estimator=clf, k_features=10, forward=True, floating=False, scoring='accuracy')

selected_features = sfs.fit_transform(dataair_x, dataair_y)

sfs.k_feature_names_

sfs.get_metric_dict()

selected_features

"""### ***3. Using the forward and Floating parameter toggle between SFS(forward True, floating False), SBS (forward False, floating False), SFFS (forward True, floating True), SBFS (forward False, floating True), and choose cross validation = 4 for each configuration. Also, report cv scores for each configuration.***"""

sfs = SFS(estimator=clf, k_features=10, forward=True, floating=False, scoring='accuracy', cv=4)
sbs = SFS(estimator=clf, k_features=10, forward=False, floating=False, scoring='accuracy', cv=4)
sffs = SFS(estimator=clf, k_features=10, forward=True, floating=True, scoring='accuracy', cv=4)
sbfs = SFS(estimator=clf, k_features=10, forward=False, floating=True, scoring='accuracy', cv=4)

sfs.fit_transform(dataair_x, dataair_y)

sbs.fit_transform(dataair_x, dataair_y)

sffs.fit_transform(dataair_x, dataair_y)

sbfs.fit_transform(dataair_x, dataair_y)

sfs_dic = sfs.get_metric_dict()
sbs_dic = sbs.get_metric_dict()
sffs_dic = sffs.get_metric_dict()
sbfs_dic = sbfs.get_metric_dict()

"""### ***4. Visualize the output from the feature selection in a pandas DataFrame format using the get_metric_dict for all four configurations. Finally, plot the results for each configuration (from mlxtend. plotting import plot_sequential_feature_selection as plot_sfs).***"""

from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs

plot_sfs(sfs_dic)

plot_sfs(sbs_dic)

plot_sfs(sffs_dic)

plot_sfs(sbfs_dic)

"""### ***5) Implement Bi-directional Feature Set Generation Algorithm from scratch. It must take a Full Set of features as well as similarity measures as input.***"""

from sklearn.feature_selection import mutual_info_classif
from sklearn.svm import SVC

class Bidirectional_FS():

    def __init__(self, df, labels, threshold=0, criterion='euclidean'):
        self.criterion = criterion
        self.df = df.T
        self.threshold = threshold
        self.labels = labels

        similarity_dict = {
            'accuracy_dtc': self.criterion_accuracy_dtc, 
            'accuracy_svm': self.criterion_accuracy_svm, 
            'euclidean': self.criterion_euclidean, 
            'city_block': self.criterion_city_block, 
            'angular': self.criterion_angular, 
            'info_gain': self.criterion_info_gain, 
        }

        self.similarity_measure = similarity_dict[self.criterion]

    def criterion_accuracy_dtc(self, X):
        clf = DTC(max_depth = 8)
        clf.fit(X.T, self.labels)
        return clf.score(X.T, self.labels)

    def criterion_accuracy_svm(self, X):
        clf = SVC()
        clf.fit(X.T, self.labels)
        return clf.score(X.T, self.labels)
        return

    def criterion_euclidean(self, X):
        dist_matrix = np.sqrt(((X[:, np.newaxis] - X)**2).sum(axis=2))
        similarity_matrix = 1 / (1 + dist_matrix)
        return np.sum(similarity_matrix)

    def criterion_city_block(self, X):
        dist_matrix = np.abs(X[:, np.newaxis] - X).sum(axis=2)
        similarity_matrix = 1 / (1 + dist_matrix)
        return np.sum(similarity_matrix)

    def criterion_angular(self, X):
        norms = np.sqrt(np.sum(X ** 2, axis=1))
        ftrs = X / norms[:, np.newaxis]
        similarity_matrix = ftrs @ ftrs.T
        return np.sum(similarity_matrix)

    def criterion_info_gain(self, X):
        info_gain_scores = mutual_info_classif(X.T, self.labels)
        info_gain_matrix = np.outer(info_gain_scores, info_gain_scores)
        return np.sum(info_gain_matrix)


    def ffs(self, Sf, best_score):
        best_feature = []

        for feature in self.df.tolist():
            if feature not in Sf:
                Sf.append(feature)
                current_score = self.similarity_measure(np.array(Sf))

                if current_score > best_score:
                    best_score = current_score
                    best_feature = feature

                Sf.remove(feature)
        Sf.append(best_feature)

        return Sf, best_score

    def bfs(self, Sb, best_score):
        worst_feature = -1
        temp = [i for i in self.df.tolist() if i not in Sb]

        for feature in range(len(temp)):

            curr = temp[feature]
            temp.remove(curr)
            current_score = self.similarity_measure(np.array(temp))

            if current_score > best_score:
                best_score = current_score
                worst_feature = feature
            temp.insert(feature, curr)

        if worst_feature == -1:
            return Sb, best_score

        curr = temp[worst_feature]

        Sb.append(curr)

        return Sb, best_score

    def bdfs(self):
        Sf = []
        Sb = []
        best_ffs_score = 0
        best_bfs_score = 0
        count = 0
        while(True):
            count += 1
            score1 = best_ffs_score
            score2 = best_bfs_score
            
            Sf, best_ffs_score = self.ffs(Sf, best_ffs_score)
            Sb, best_bfs_score = self.bfs(Sb, best_bfs_score)
            print(f'Iteration {count} ----- ')
            print(f'best_ffs_score = {best_ffs_score}')
            print(f'best_bfs_score = {best_bfs_score}')
            print()

            temp = [i for i in self.df.tolist() if i not in Sb]
            
            if self.threshold != 0:

                if best_bfs_score > self.threshold:
                    return temp

                elif best_ffs_score > self.threshold:
                    return Sf

            if (score1 == best_ffs_score) or (score2 == best_bfs_score):
                if score1 > score2:
                    return Sf
                else:
                    return temp

            elif (len(self.df) == len(Sf)) or len(temp)==0:
                return Sf

"""### ***6) Use the function implemented in part 5 and use selection criteria from the following:***

***Accuracy Measures: using Decision Tree and SVM Classifiers***

***Information Measures: Information gain***

***Distance Measure: Angular Separation, Euclidian Distance and City-Block Distance***

***Distance Measures. - Measures of separability, discrimination or divergence measures. The most typical is derived from the distance between the class conditional density functions.)***
"""

model = Bidirectional_FS(dataair_x.to_numpy(), dataair_y.to_numpy(), threshold=300, criterion='city_block')
ctblk_x = model.bdfs()

model = Bidirectional_FS(dataair_x.to_numpy(), dataair_y.to_numpy(), threshold=300, criterion='euclidean')
euclid_x = model.bdfs()

model = Bidirectional_FS(dataair_x.to_numpy(), dataair_y.to_numpy(), threshold=300, criterion='angular')
ang_x = model.bdfs()

model = Bidirectional_FS(dataair_x.to_numpy(), dataair_y.to_numpy(), threshold=0.4, criterion='info_gain')
infog_x = model.bdfs()

model = Bidirectional_FS(dataair_x.to_numpy(), dataair_y.to_numpy(), threshold=0.8, criterion='accuracy_svm')
svm_x = model.bdfs()

model = Bidirectional_FS(dataair_x.to_numpy(), dataair_y.to_numpy(), threshold=0.9, criterion='accuracy_dtc')
dtc_x = model.bdfs()

"""### ***7) Train any classifier of your choice on the Selected features generated from each measure and report its classification results.***"""

from sklearn.tree import DecisionTreeClassifier as DTC

clf = DTC(max_depth=5)
clf.fit(np.array(ctblk_x).T, dataair_y)
clf.score(np.array(ctblk_x).T, dataair_y)

clf = DTC(max_depth=5)
clf.fit(np.array(euclid_x).T, dataair_y)
clf.score(np.array(euclid_x).T, dataair_y)

clf = DTC(max_depth=5)
clf.fit(np.array(ang_x).T, dataair_y)
clf.score(np.array(ang_x).T, dataair_y)

"""# ***QUESTION 2***

---



---

### ***1. Make a Dataset of 1000 points sampled from a zero-centred gaussian distribution with a covariance matrix.***
"""

import pandas as pd
import numpy as np

covar_mat = np.matrix('0.6006771 0.14889879 0.244939; 0.14889879 0.58982531 0.241548981; 0.244939 0.24154981 0.48778655')
mean = [0, 0, 0]

data_2 = np.random.multivariate_normal(mean, covar_mat, size=1000)

labels = np.zeros(1000)
v = np.array([1, 1, -2]) / np.sqrt(6)

for i in range(1000):
    if np.dot(data_2[i], v) > 0:
        labels[i] = 0
    else:
        labels[i] = 1

data_df_2 = pd.DataFrame(data_2)
data_df_2[3] = labels
data_df_2.columns = ['x', 'y', 'z', 'labels']

import plotly.express as px

fig = px.scatter_3d(data_df_2, x='x', y='y', z='z',
              color='labels')
fig.show()

"""### ***2. Apply Principal Component analysis (using sklearn) with n_components=3 on the input data X and transform the data accordingly.***"""

from sklearn.decomposition import PCA

pca = PCA(n_components=3)
data2_pca = pca.fit_transform(data_2)

data2_pca

data2_df_pca = pd.DataFrame(data2_pca)
data2_df_pca[3] = labels
data2_df_pca.columns = ['x', 'y', 'z', 'labels']

"""### ***3. Perform Complete FS on the Transformed Data with a number of features in subset =2. Fit a Decision Tree for every subset-set of features of size 2 and plot their decision boundaries superimposed with the data.***"""

from sklearn.tree import DecisionTreeClassifier as DTC
import sklearn.feature_selection

features = np.array([['x', 'z'],
                     ['y', 'z'],
                     ['x', 'y']])

import matplotlib.pyplot as plt
from sklearn.inspection import DecisionBoundaryDisplay
from sklearn.model_selection import train_test_split as split

plt.figure(figsize=(16, 5))

for i in range(3):

    feature = features[i]
    classes = [0, 1]

    x = pd.DataFrame.to_numpy(data2_df_pca[feature])

    x_train, x_test, y_train, y_test = split(x, labels, train_size = 0.8)

    dtc = DTC()
    dtc.fit(x_train, y_train)

    print("\n\nFor feature axis ", feature, ', accuracy on train data is: ', dtc.score(x_train, y_train), '...............and on test data is: ', dtc.score(x_test, y_test), '\n\n')

    # Parameters
    n_classes = 2
    plot_colors = "rb"
    plot_step = 0.02

    ax = plt.subplot(1, 3, i+1)

    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)
    DecisionBoundaryDisplay.from_estimator(
        dtc,
        x_train,
        cmap=plt.cm.RdYlBu,
        response_method="predict",
        ax=ax,
        xlabel=feature[0],
        ylabel=feature[1],
        alpha = 0.6
        )

    # Plot the training points
    for i, color in zip(range(n_classes), plot_colors):
        idx = np.where(y_train == classes[i])
        plt.scatter(
            x_train[idx, 0],
            x_train[idx, 1],
            c=color,
            cmap=plt.cm.RdYlBu,
            edgecolor="black",
            s=15,
        )

plt.show()

"""### ***4. Which of the above feature subsets represents the one that can be obtained by applying PCA(n_components =2)? Explain the difference in the accuracies between this subset and other subsets by running suitable experiments.***"""

pca = PCA(n_components=2)
data2_pca_4 = pca.fit_transform(data_2)

plt.scatter(data2_pca_4[:, 0], data2_pca_4[:, 1], c=labels, cmap='RdYlBu')
plt.show()

"""**The data formed with n_components=2 can represent the subset which had 'x' and 'y' axis of the data as its features from the previous data with n_components=3.**"""

x = data2_pca_4
x_train2, x_test2, y_train2, y_test2 = split(x, labels, train_size = 0.8)

dtc2 = DTC()
dtc2.fit(x_train2, y_train2)

plt.figure(figsize=(20, 12))

# Parameters
n_classes = 2
plot_colors = "rb"
plot_step = 0.02

ax = plt.subplot(2, 3, 1)

plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)
DecisionBoundaryDisplay.from_estimator(
    dtc2,
    x_train2,
    cmap=plt.cm.RdYlBu,
    response_method="predict",
    ax=ax,
    xlabel='x',
    ylabel='y',
    alpha = 0.6
    )

# Plot the training points
for i, color in zip(range(n_classes), plot_colors):
    idx = np.where(y_train2 == classes[i])
    plt.scatter(
        x_train2[idx, 0],
        x_train2[idx, 1],
        c=color,
        cmap=plt.cm.RdYlBu,
        edgecolor="black",
        s=15,
    )

plt.show()

print('Accuracy on train data is: ', dtc2.score(x_train2, y_train2), '   and on test data is: ', dtc2.score(x_test2, y_test2))